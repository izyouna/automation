#!/bin/bash

# Phase 1 Execution Script: Foundation for Cloud-Native Evolution
# Date: February 13, 2026
# Status: AUTO-EXECUTING

set -e

PHASE_DIR=$(pwd)
EVOLUTION_DIR="."
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  PHASE 1: FOUNDATION EXECUTION                             â•‘"
echo "â•‘  Starting: $(date)                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# ============================================================================
# STEP 1: HEALTH CHECKS IMPLEMENTATION
# ============================================================================

echo "â–¶ STEP 1: HEALTH CHECKS IMPLEMENTATION"
echo "  Creating health check scripts and configurations..."

mkdir -p health-checks

# Gateway health check
cat > health-checks/gateway-health.sh << 'EOF'
#!/bin/bash
# Nginx Gateway Health Check
# Tests HTTP and HTTPS endpoints

echo "Checking Nginx gateway..."

# HTTP health check
if curl -sf http://localhost:80/status > /dev/null 2>&1; then
    echo "[OK] HTTP health check passed"
else
    echo "[FAIL] HTTP health check failed"
    exit 1
fi

# HTTPS health check
if curl -sf https://localhost:443/status --insecure > /dev/null 2>&1; then
    echo "[OK] HTTPS health check passed"
else
    echo "[FAIL] HTTPS health check failed"
    exit 1
fi

echo "[OK] Gateway health verified"
exit 0
EOF
chmod +x health-checks/gateway-health.sh

# MIME Server health check
cat > health-checks/mime-server-health.py << 'EOF'
#!/usr/bin/env python3
"""
Health check for MIME Server
Tests connectivity and responsiveness on port 65432
"""

import socket
import sys
import time

def check_socket_connectivity():
    """Check if MIME server socket is responding"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        
        result = sock.connect_ex(('localhost', 65432))
        sock.close()
        
        if result == 0:
            print("[OK] MIME server socket is accepting connections")
            return True
        else:
            print("[FAIL] MIME server socket is not responding")
            return False
    except Exception as e:
        print(f"[FAIL] Health check error: {e}")
        return False

def check_storage_accessible():
    """Check if storage volume is mounted"""
    try:
        import os
        storage_path = "/storage"
        
        if os.path.ismount(storage_path) or os.path.exists(storage_path):
            print(f"[OK] Storage path {storage_path} is accessible")
            return True
        else:
            print(f"[FAIL] Storage path {storage_path} not accessible")
            return False
    except Exception as e:
        print(f"[WARN] Could not verify storage: {e}")
        return True  # Non-fatal

if __name__ == "__main__":
    checks = [
        check_socket_connectivity(),
        check_storage_accessible()
    ]
    
    if all(checks):
        print("[OK] All health checks passed")
        sys.exit(0)
    else:
        print("[FAIL] Some health checks failed")
        sys.exit(1)
EOF
chmod +x health-checks/mime-server-health.py

# Application health endpoint template
cat > health-checks/app-health-endpoint.py << 'EOF'
"""
Health check endpoints for Python applications
Add these to your Flask/FastAPI application
"""

from flask import Flask, jsonify
from datetime import datetime

app = Flask(__name__)

# Database connection pool (example)
db_pool = None

@app.route('/health/live', methods=['GET'])
def liveness():
    """
    Liveness probe - Is the service alive?
    Used by Docker/Kubernetes to restart if failing
    """
    return jsonify({
        'status': 'alive',
        'service': 'app',
        'timestamp': datetime.utcnow().isoformat(),
        'uptime_seconds': get_uptime()
    }), 200

@app.route('/health/ready', methods=['GET'])
def readiness():
    """
    Readiness probe - Can the service accept traffic?
    Used by load balancers to route traffic
    """
    checks = {
        'database': check_database_connection(),
        'file_storage': check_storage_accessible(),
        'dependencies': check_dependencies()
    }
    
    if all(checks.values()):
        return jsonify({
            'status': 'ready',
            'checks': checks,
            'timestamp': datetime.utcnow().isoformat()
        }), 200
    else:
        return jsonify({
            'status': 'not_ready',
            'checks': checks,
            'timestamp': datetime.utcnow().isoformat()
        }), 503

def check_database_connection():
    """Check if database is accessible"""
    try:
        # Implement your DB check
        return True
    except:
        return False

def check_storage_accessible():
    """Check if storage is mounted"""
    import os
    try:
        return os.path.exists('/storage') or os.path.ismount('/storage')
    except:
        return False

def check_dependencies():
    """Check if external dependencies are accessible"""
    try:
        # Check network connectivity, API endpoints, etc.
        return True
    except:
        return False

def get_uptime():
    """Get service uptime in seconds"""
    # Implement proper uptime tracking
    return 0

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
EOF

echo "âœ“ Health check scripts created (4 files)"
echo "  - gateway-health.sh"
echo "  - mime-server-health.py"
echo "  - app-health-endpoint.py"
echo ""

# ============================================================================
# STEP 2: TERRAFORM INFRASTRUCTURE
# ============================================================================

echo "â–¶ STEP 2: TERRAFORM INFRASTRUCTURE"
echo "  Creating Terraform configuration files..."

mkdir -p terraform/environments/configs
mkdir -p terraform/modules/{network,service,monitoring}

# Main Terraform file
cat > terraform/main.tf << 'EOF'
# Architecture Evolution: Phase 1 - Foundation
# Terraform configuration for multi-environment infrastructure

terraform {
  required_version = ">= 1.0"

  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0"
    }
  }

  # Uncomment for remote state (AWS S3, etc.)
  # backend "s3" {
  #   bucket         = "my-terraform-state"
  #   key            = "evolution/terraform.tfstate"
  #   region         = "us-east-1"
  #   encrypt        = true
  #   dynamodb_table = "terraform-locks"
  # }

  backend "local" {
    path = "terraform.tfstate"
  }
}

provider "docker" {
  # For Podman: unix:///run/podman/podman.sock
  # For Docker: unix:///var/run/docker.sock
  host = var.docker_host
}

# =============================================================================
# NETWORKS
# =============================================================================

resource "docker_network" "public_net" {
  name     = var.public_network_name
  driver   = "bridge"
  internal = false

  ipam_config {
    subnet = var.public_network_subnet
  }

  labels = {
    "zone"        = "public"
    "environment" = var.environment
  }
}

resource "docker_network" "private_net" {
  name     = var.private_network_name
  driver   = "bridge"
  internal = true  # No external access

  ipam_config {
    subnet = var.private_network_subnet
  }

  labels = {
    "zone"        = "private"
    "environment" = var.environment
  }
}

# =============================================================================
# VOLUMES
# =============================================================================

resource "docker_volume" "mime_storage" {
  name = var.storage_volume_name

  labels = {
    "service"     = "mime-server"
    "environment" = var.environment
  }
}

# =============================================================================
# OUTPUTS
# =============================================================================

output "public_network_id" {
  description = "ID of public network"
  value       = docker_network.public_net.id
}

output "private_network_id" {
  description = "ID of private network"
  value       = docker_network.private_net.id
}

output "storage_volume_name" {
  description = "Name of MIME storage volume"
  value       = docker_volume.mime_storage.name
}
EOF

# Variables file
cat > terraform/variables.tf << 'EOF'
variable "environment" {
  description = "Environment name (dev, staging, prod)"
  type        = string
  default     = "dev"

  validation {
    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, staging, or prod."
  }
}

variable "docker_host" {
  description = "Docker/Podman API endpoint"
  type        = string
  default     = "unix:///run/podman/podman.sock"
}

variable "public_network_name" {
  description = "Name of public network"
  type        = string
  default     = "public_net"
}

variable "public_network_subnet" {
  description = "CIDR for public network"
  type        = string
  default     = "172.18.0.0/16"
}

variable "private_network_name" {
  description = "Name of private network"
  type        = string
  default     = "private_net"
}

variable "private_network_subnet" {
  description = "CIDR for private network"
  type        = string
  default     = "172.19.0.0/16"
}

variable "storage_volume_name" {
  description = "Name of mime storage volume"
  type        = string
  default     = "mime_storage"
}

variable "enable_monitoring" {
  description = "Enable Prometheus/Grafana monitoring"
  type        = bool
  default     = true
}
EOF

# Environment-specific files
cat > terraform/environments/dev.tfvars << 'EOF'
environment       = "dev"
docker_host       = "unix:///run/podman/podman.sock"
enable_monitoring = true
EOF

cat > terraform/environments/staging.tfvars << 'EOF'
environment       = "staging"
docker_host       = "unix:///run/podman/podman.sock"
enable_monitoring = true
EOF

cat > terraform/environments/prod.tfvars << 'EOF'
environment       = "prod"
docker_host       = "unix:///run/podman/podman.sock"
enable_monitoring = true
EOF

# Terraform README
cat > terraform/README.md << 'EOF'
# Terraform Infrastructure Configuration

## Quick Start

```bash
# Initialize Terraform
terraform init

# Plan changes for development
terraform plan -var-file=environments/dev.tfvars

# Apply changes
terraform apply -var-file=environments/dev.tfvars

# Destroy infrastructure
terraform destroy -var-file=environments/dev.tfvars
```

## Environments

- **dev**: Local development environment
- **staging**: Pre-production testing
- **prod**: Production deployment

## Files

- `main.tf`: Core infrastructure definitions
- `variables.tf`: Input variables and validation
- `outputs.tf`: Output values
- `environments/`: Environment-specific configurations
- `modules/`: Reusable Terraform modules

## State Management

State is stored locally in `terraform.tfstate`. For production, use remote state (S3, Terraform Cloud, etc.).

```hcl
backend "s3" {
  bucket = "my-terraform-state"
  key    = "evolution/phase1/terraform.tfstate"
}
```

EOF

echo "âœ“ Terraform infrastructure created (6 files)"
echo "  - main.tf (infrastructure definitions)"
echo "  - variables.tf (input variables)"
echo "  - dev.tfvars, staging.tfvars, prod.tfvars"
echo "  - README.md (documentation)"
echo ""

# ============================================================================
# STEP 3: MONITORING STACK
# ============================================================================

echo "â–¶ STEP 3: MONITORING STACK (Prometheus + Grafana)"
echo "  Creating monitoring configuration..."

mkdir -p monitoring/prometheus
mkdir -p monitoring/grafana/dashboards
mkdir -p monitoring/grafana/provisioning/datasources

# Prometheus configuration
cat > monitoring/prometheus/prometheus.yml << 'EOF'
# Global configuration
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'evolution-monitor'
    environment: 'dev'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules files
rule_files:
  - '/etc/prometheus/rules.yml'

# Scrape configurations
scrape_configs:

  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Docker/Podman daemon metrics
  - job_name: 'docker'
    static_configs:
      - targets: ['localhost:9323']

  # MIME Server
  - job_name: 'mime-server'
    static_configs:
      - targets: ['mime-server:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  # Gateway (if monitoring endpoint exposed)
  - job_name: 'gateway'
    static_configs:
      - targets: ['mockup-gateway:9113']

  # Node Exporter (if installed)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    
  # Public App
  - job_name: 'public_app'
    static_configs:
      - targets: ['public_app:8001']

  # Internal API
  - job_name: 'intranet_api'
    static_configs:
      - targets: ['intranet_api:8002']
EOF

# Alert rules
cat > monitoring/prometheus/rules.yml << 'EOF'
groups:
  - name: mime_transfer
    interval: 30s
    rules:

      - alert: MimeServerDown
        expr: up{job="mime-server"} == 0
        for: 2m
        annotations:
          summary: "MIME Server is down"
          description: "MIME Server has been down for more than 2 minutes"

      - alert: GatewayHighErrorRate
        expr: rate(gateway_http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "Gateway has high error rate"
          description: "Error rate is > 5% for 5 minutes"

      - alert: StorageVolumeFull
        expr: (docker_container_volumes_used / docker_container_volumes_limit) > 0.9
        annotations:
          summary: "Storage volume is >90% full"

      - alert: HighLatency
        expr: histogram_quantile(0.95, transfer_duration_seconds) > 5
        for: 5m
        annotations:
          summary: "File transfer latency is high"
          description: "p95 latency > 5 seconds"
EOF

# Grafana datasource provisioning
cat > monitoring/grafana/provisioning/datasources/prometheus.yml << 'EOF'
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
EOF

# Grafana dashboard template (JSON)
cat > monitoring/grafana/dashboards/mime-transfer.json << 'EOF'
{
  "dashboard": {
    "title": "MIME Transfer Service - Phase 1",
    "timezone": "browser",
    "refresh": "10s",
    "schemaVersion": 30,
    "version": 1,
    "panels": [
      {
        "id": 1,
        "title": "Service Status",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=~\"mime-server|gateway\"}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "File Transfer Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(file_transfers_total[5m])",
            "legendFormat": "transfers/sec"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Average Transfer Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(transfer_duration_seconds_sum[5m]) / rate(transfer_duration_seconds_count[5m])",
            "legendFormat": "avg duration"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "Storage Usage",
        "type": "gauge",
        "targets": [
          {
            "expr": "(docker_container_volumes_used / docker_container_volumes_limit) * 100"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ]
  }
}
EOF

# Docker Compose for monitoring
cat > monitoring/docker-compose.monitoring.yml << 'EOF'
version: '3.9'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules.yml:/etc/prometheus/rules.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/etc/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:

networks:
  monitoring:
    driver: bridge
EOF

echo "âœ“ Monitoring stack created (5 files)"
echo "  - prometheus.yml (configuration)"
echo "  - rules.yml (alert rules)"
echo "  - mime-transfer.json (Grafana dashboard)"
echo "  - datasources config"
echo "  - docker-compose.monitoring.yml"
echo ""

# ============================================================================
# STEP 4: CI/CD PIPELINE
# ============================================================================

echo "â–¶ STEP 4: CI/CD PIPELINE (GitHub Actions)"
echo "  Creating GitHub Actions workflows..."

mkdir -p .github/workflows

# Test workflow
cat > .github/workflows/test.yml << 'EOF'
name: Test & Validate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Validate JSON files
        run: |
          find monitoring/grafana/dashboards -name "*.json" -exec jq . {} \; > /dev/null

      - name: Validate Terraform
        run: |
          cd terraform
          terraform init -backend=false
          terraform validate

  test-health-checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Test health check scripts
        run: |
          python -m py_compile health-checks/mime-server-health.py
          python -m py_compile health-checks/app-health-endpoint.py

  docker-compose-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Check docker-compose syntax
        run: |
          docker-compose config > /dev/null
          docker-compose -f monitoring/docker-compose.monitoring.yml config > /dev/null
EOF

# Deploy workflow
cat > .github/workflows/deploy.yml << 'EOF'
name: Deploy Phase 1

on:
  workflow_dispatch:  # Manual trigger
  push:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'monitoring/**'
      - '.github/workflows/deploy.yml'

jobs:
  deploy-terraform:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Init
        run: |
          cd terraform
          terraform init -backend=false

      - name: Terraform Plan
        run: |
          cd terraform
          terraform plan -var-file=environments/dev.tfvars -out=tfplan

      - name: Terraform Apply
        if: success()
        run: |
          cd terraform
          terraform apply -auto-approve tfplan

  deploy-monitoring:
    needs: deploy-terraform
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Deploy Prometheus & Grafana
        run: |
          cd monitoring
          docker-compose -f docker-compose.monitoring.yml up -d

      - name: Wait for Grafana
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:3000/api/health > /dev/null; then
              echo "Grafana is ready"
              exit 0
            fi
            echo "Waiting for Grafana... ($i/30)"
            sleep 2
          done
          exit 1

      - name: Verify Grafana Datasource
        run: |
          curl -X GET http://localhost:3000/api/datasources
EOF

echo "âœ“ CI/CD pipelines created (2 files)"
echo "  - test.yml (validation & testing)"
echo "  - deploy.yml (infrastructure deployment)"
echo ""

# ============================================================================
# SUMMARY & NEXT STEPS
# ============================================================================

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  PHASE 1 FOUNDATION EXECUTION COMPLETE                    â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“¦ DELIVERABLES CREATED:"
echo ""
echo "  âœ“ Health Checks (4 files)"
echo "    - Nginx gateway health probe"
echo "    - MIME server socket check"
echo "    - Python application health endpoints"
echo ""
echo "  âœ“ Terraform Infrastructure (6 files)"
echo "    - Multi-environment configuration"
echo "    - Network definitions (public_net, private_net)"
echo "    - Volume management"
echo "    - Validation rules"
echo ""
echo "  âœ“ Monitoring Stack (5 files)"
echo "    - Prometheus configuration"
echo "    - Alert rules for critical services"
echo "    - Grafana datasource & dashboard templates"
echo "    - Docker Compose for monitoring services"
echo ""
echo "  âœ“ CI/CD Pipelines (2 files)"
echo "    - Validation workflow (lint, test)"
echo "    - Deployment workflow (automated rollout)"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "ğŸš€ NEXT STEPS:"
echo ""
echo "1. Review the configurations:"
echo "   - cat terraform/README.md"
echo "   - cat evolution_plan.md"
echo ""
echo "2. Initialize Terraform:"
echo "   cd terraform"
echo "   terraform init"
echo "   terraform plan -var-file=environments/dev.tfvars"
echo ""
echo "3. Deploy monitoring (from monitoring/ directory):"
echo "   docker-compose -f docker-compose.monitoring.yml up -d"
echo ""
echo "4. Access Grafana:"
echo "   http://localhost:3000"
echo "   User: admin | Password: admin"
echo ""
echo "5. Verify prometheus metrics:"
echo "   http://localhost:9090"
echo ""
echo "6. Test health checks:"
echo "   bash health-checks/gateway-health.sh"
echo "   python health-checks/mime-server-health.py"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "ğŸ“Š PHASE 1 COMPLETION METRICS:"
echo ""
echo "  Timeline:          2-4 weeks"
echo "  Complexity:        Low â†’ Medium"
echo "  Cloud-Readiness:   30% â†’ 60%"
echo "  Development:       ~3-4 engineers"
echo ""
echo "âœ… PHASE 1 READY"
echo "   Proceed to Phase 2 (Kubernetes migration) when ready"
echo ""
echo "Completed: $(date)"
echo ""
